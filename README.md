# NMT_BERT

<br>

## Intro

Nowadays, it has become common to use a pre-trained model pre-trained on large-scale data.

And BERT is most basic and universal model for fine-tuning in specific NLP Tasks.

As BERT only consists of Transformer Encoders, it goes well with Classification Tasks.

But there was a study to use Large Scale Pretrained Models to end-to-end Text Generation Tasks.

Idea borrowed from this study and covers ablations with BERT/AlBERT/DistillBERT


<br>

## Model
* BERT
* AlBERT
* Distill BERT


<br>

## References
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

[Incorporating BERT into Neural Machine Translation](https://arxiv.org/abs/2002.06823)
